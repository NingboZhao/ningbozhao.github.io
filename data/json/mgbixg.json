{
  "body_html": "<h2 id=\"LQYni\">一、K-均值聚类（K-means）概述</h2><p><br /></p><h4 id=\"fICZE\">1. 聚类</h4><p>“类”指的是具有相似性的集合。聚类是指将数据集划分为若干类，使得类内之间的数据最为相似，各类之间的数据相似度差别尽可能大。聚类分析就是以相似性为基础，对数据集进行聚类划分，属于无监督学习。</p><p><br /></p><h4 id=\"QoqDp\">2. 无监督学习和监督学习</h4><p>和KNN所不同，K-均值聚类属于无监督学习。那么监督学习和无监督学习的区别在哪儿呢？监督学习知道从对象（数据）中学习什么，而无监督学习无需知道所要搜寻的目标，它是根据算法得到数据的共同特征。比如用分类和聚类来说，分类事先就知道所要得到的类别，而聚类则不一样，只是以相似度为基础，将对象分得不同的簇。</p><p><br /></p><h4 id=\"fPkkM\">3. K-means</h4><p>k-means算法是一种简单的迭代型聚类算法，采用距离作为相似性指标，从而发现给定数据集中的K个类，且每个类的中心是根据类中所有值的均值得到，每个类用聚类中心来描述。对于给定的一个包含n个d维数据点的数据集X以及要分得的类别K,选取欧式距离作为相似度指标，聚类目标是使得各类的聚类平方和最小，即最小化：</p><p><img alt=\"image.png\" title=\"image.png\" src=\"https://cdn.nlark.com/yuque/0/2019/png/85998/1558429325812-de8883f2-3029-4070-b2e3-add601eff17d.png#align=left&amp;display=inline&amp;height=51&amp;name=image.png&amp;originHeight=101&amp;originWidth=261&amp;size=9120&amp;status=done&amp;width=130.5\" style=\"max-width: 600px; width: 130.5px;\" /></p><p><br /></p><p>结合最小二乘法和拉格朗日原理，聚类中心为对应类别中各数据点的平均值，同时为了使得算法收敛，在迭代过程中，应使最终的聚类中心尽可能的不变。</p><p><br /></p><h4 id=\"Fd85i\">4. 算法流程</h4><p>K-means是一个反复迭代的过程，算法分为四个步骤：</p><p>1） 选取数据空间中的K个对象作为初始中心，每个对象代表一个聚类中心；</p><p>2） 对于样本中的数据对象，根据它们与这些聚类中心的欧氏距离，按距离最近的准则将它们分到距离它们最近的聚类中心（最相似）所对应的类；</p><p>3） 更新聚类中心：将每个类别中所有对象所对应的均值作为该类别的聚类中心，计算目标函数的值；</p><p>4） 判断聚类中心和目标函数的值是否发生改变，若不变，则输出结果，若改变，则返回2）。</p><p><br /></p><p>用以下例子加以说明：</p><p><img alt=\"image.png\" title=\"image.png\" src=\"https://cdn.nlark.com/yuque/0/2019/png/85998/1558429367312-388b8ee1-40a1-4897-be31-9a43049573e3.png#align=left&amp;display=inline&amp;height=590&amp;name=image.png&amp;originHeight=1180&amp;originWidth=1100&amp;size=719115&amp;status=done&amp;width=550\" style=\"max-width: 600px; width: 550px;\" /></p><p><br />图1：给定一个数据集；</p><p>图2：根据K = 5初始化聚类中心，保证　聚类中心处于数据空间内；</p><p>图3：根据计算类内对象和聚类中心之间的相似度指标，将数据进行划分；</p><p>图4：将类内之间数据的均值作为聚类中心，更新聚类中心。</p><p>最后判断算法结束与否即可，目的是为了保证算法的收敛。</p><p><br /></p><h2 id=\"M2hpj\">二、Python 实现（过程忽略）</h2><p><br /></p><p><img alt=\"image.png\" title=\"image.png\" src=\"https://cdn.nlark.com/yuque/0/2019/png/85998/1558429456239-f0fdaf09-ab3c-4168-8068-ba5d5bddf7a2.png#align=left&amp;display=inline&amp;height=210&amp;name=image.png&amp;originHeight=419&amp;originWidth=564&amp;size=111859&amp;status=done&amp;width=282\" style=\"max-width: 600px; width: 282px;\" /><img alt=\"image.png\" title=\"image.png\" src=\"https://cdn.nlark.com/yuque/0/2019/png/85998/1558429461008-3b9ad602-522e-4f4b-8edc-c1e52d9cb29d.png#align=left&amp;display=inline&amp;height=211&amp;name=image.png&amp;originHeight=422&amp;originWidth=565&amp;size=142383&amp;status=done&amp;width=282.5\" style=\"max-width: 600px; width: 282.5px;\" /></p><p><br /></p><p>总结：在这次程序的调试中，其实出现的问题还是蛮多的，相似度指标依旧选用的是欧氏距离。在之前，一直是按照公式直接计算的，可欧氏距离其实就是2范数啊，2范数属于酉不变范数，因此矩阵的2范数就是矩阵的最大奇异值，在求解过程中可以直接采用norm函数简化。</p><p><br /></p><h4 id=\"pvuvE\">模型检验</h4><p>上图中的结果可以清晰的看到算法具有一定的聚类效果，要进一步验证的话，可以采取MCR或者NMI和ARI这些常用的准则进行衡量聚类结果的优劣，在此我选取MCR进行验证，代码如下：</p><p><br /></p><pre data-lang=\"python\"><code>%% 采用MCR判定聚类效果\n B = class(:,4);\n B = reshape(B,1,row);\n A = [ones(1,100),2 * ones(1,100),3 *ones(1,100),4 * ones(1,100)];\n \nsum = 0;\nfor i = 1:row\n    if ( A(1,i) ~= B(1,i))\n        sum = sum + 1;\n    end\nend\nMCR = sum / row;\nfprintf('MCR = %d\\n',MCR);</code></pre><p><br /></p><p><br /></p><p>多次计算平均求得的MCR= 0.53,表明误分率还是蛮大的，聚类效果并不是很理想，究其原因：虽然算法收敛，但算法只是收敛到了局部最小值，而并非全局最小值，所以可以引入二分K-均值对算法进行优化。</p><p>除此之外，FCM算法在一定程度上也是对算法的一个优化吧。</p><p><br /></p><p>原文：<a href=\"https://www.cnblogs.com/ybjourney/p/4714870.html\" target=\"_blank\">https://www.cnblogs.com/ybjourney/p/4714870.html</a></p>",
  "slug": 1731898,
  "title": "K-means 聚类：理论"
}