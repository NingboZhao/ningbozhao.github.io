{
  "body_html": "<p style=\"text-align: justify;\"><span class=\"lake-fontsize-11\">K-均值的意思是它可以发现K个不同的组类，且每个组类的中心采用组中所包含的值的均值计算而成，也就是选取</span><strong><span class=\"lake-fontsize-11\">每组数据的平均值</span></strong><span class=\"lake-fontsize-11\">作为数据的“中心”. 因此，我们首先需要组类识别，它与传统意义的分类不同，分类是我们事先知道它是属于哪一类，也就是像SVM之类的监督型算法，聚类是没有事先定义，就像我们不知道自己未来会分成哪几类人.</span></p><p><br /></p><p style=\"text-align: justify;\"><span class=\"lake-fontsize-11\">因此聚类也被称为无监督分类. K-均值算法的工作流程非常简单大致如下：</span></p><ol style=\"text-align: justify;\"><li><span class=\"lake-fontsize-11\">挑选K个初始点作为起始的质心（也就是中心点，一般随机选择）</span><br /></li><li><span class=\"lake-fontsize-11\">为数据集中的每个点找到距离它最近的质心，并把这个点分给这个组类（去质心那里拜码头）</span><br /></li><li><span class=\"lake-fontsize-11\">将每个组类的所有点进行取平均值作为新的质心（重新挑老大）</span><br /></li></ol><p><br /></p><p style=\"text-align: justify;\"><span class=\"lake-fontsize-11\">从上面三个步骤我们可以看到，“最近”的质心，也就是需要进行</span><strong><span class=\"lake-fontsize-11\">距离计算</span></strong><span class=\"lake-fontsize-11\">，当然使用不同的距离计算方法，得到的聚类效果也是不同的。</span></p><p><span class=\"lake-fontsize-11\"><br /></span></p><p style=\"text-align: justify;\"><span class=\"lake-fontsize-11\">K-均值优点是容易实现，就是取平均值嘛；缺点是在处理大规模数据时候收敛速度较慢；适合的数据类型：数值型数据。</span></p><p><br /></p><p style=\"text-align: justify;\"><span class=\"lake-fontsize-11\">既然聚类分析是按照对象之间的相似性和相异性来聚，那么如何来刻画对象的相似（相异）程度呢？这就今天要介绍的尺子——距离（distance）。</span></p><p><span class=\"lake-fontsize-11\"><br /></span></p><p style=\"text-align: justify;\"><span class=\"lake-fontsize-11\">距离是两对象之间的时间或空间相隔远近的度量，记d(A,B)为对象A与对象B之间的距离，距离要满足下面3条公理.</span></p><ol style=\"text-align: justify;\"><li><span class=\"lake-fontsize-11\">非负性, 即 d(A,B)≥0，当且仅当A和B在一起的时候，d(A,B)=0;</span><br /></li><li><span class=\"lake-fontsize-11\">对称性，即d(A,B)=d(B,A) ,  也就是从A到B的距离与从B到A的距离是相等的.</span><br /></li><li><span class=\"lake-fontsize-11\">三角不等式性，即d(A,B)≤d(A,C)+d(C,B)  如果在平面上很好理解，三角形两边之和不小于第三边.</span><br /></li></ol><p><br /></p><p style=\"text-align: justify;\"><span class=\"lake-fontsize-11\">满足这3条公理的距离也有好几种，先从最简单的入手，假设有n个样本，每个样本有k个指标，数据矩阵如下表所示</span></p><p><br /></p><p style=\"text-align: center;\"><img alt=\"image.png\" title=\"image.png\" src=\"https://cdn.nlark.com/yuque/0/2019/png/85998/1549871032699-cb714d10-fee2-4683-aa50-8e26e35a8ada.png#align=left&amp;display=inline&amp;height=187&amp;name=image.png&amp;originHeight=374&amp;originWidth=918&amp;size=69655&amp;width=459\" style=\"max-width: 600px; width: 459px;\" /></p><p><br /></p><p><span class=\"lake-fontsize-11\" style=\"color: #333333;\">第i个样本为<img alt=\"image.png\" title=\"image.png\" src=\"https://cdn.nlark.com/yuque/0/2019/png/85998/1549871051211-629da3d8-acdb-43be-972f-1b759181f5bc.png#align=left&amp;display=inline&amp;height=24&amp;name=image.png&amp;originHeight=22&amp;originWidth=154&amp;size=3783&amp;width=168\" style=\"max-width: 600px; width: 168px;\" /></span></p><p><span class=\"lake-fontsize-11\" style=\"color: #333333;\">第j个样本为<img alt=\"image.png\" title=\"image.png\" src=\"https://cdn.nlark.com/yuque/0/2019/png/85998/1549871092026-e850ee61-b167-4627-81ee-b2c4bc1bf1df.png#align=left&amp;display=inline&amp;height=24&amp;name=image.png&amp;originHeight=22&amp;originWidth=160&amp;size=3747&amp;width=175\" style=\"max-width: 600px; width: 175px;\" /></span></p><p><span class=\"lake-fontsize-11\" style=\"color: #333333;\">把每个样本看成p维空间里的质点，记（i）与（j）的距离为d(i,j) ， 那么</span></p><h4 id=\"f83dad23\">绝对距离</h4><p style=\"text-align: center;\"><span class=\"lake-fontsize-11\" style=\"color: #333333;\"><img alt=\"image.png\" title=\"image.png\" src=\"https://cdn.nlark.com/yuque/0/2019/png/85998/1549871124344-6da781eb-38e9-4113-9bdc-077301d10172.png#align=left&amp;display=inline&amp;height=43&amp;name=image.png&amp;originHeight=52&amp;originWidth=290&amp;size=9452&amp;width=238\" style=\"max-width: 600px; width: 238px;\" /></span></p><h4 id=\"b87434b5\">欧几里得距离</h4><p style=\"text-align: center;\"><span class=\"lake-fontsize-11\" style=\"color: #333333;\"><img alt=\"image.png\" title=\"image.png\" src=\"https://cdn.nlark.com/yuque/0/2019/png/85998/1549871269283-2e09702c-a43b-4486-b1d8-7f38ae56acd9.png#align=left&amp;display=inline&amp;height=49&amp;name=image.png&amp;originHeight=66&amp;originWidth=335&amp;size=10825&amp;width=250\" style=\"max-width: 600px; width: 250px;\" /></span></p><h4 id=\"da71f1b4\">闵科夫斯基距离</h4><p style=\"text-align: center;\"><img alt=\"image.png\" title=\"image.png\" src=\"https://cdn.nlark.com/yuque/0/2019/png/85998/1549871293883-f345a2a6-a62d-4379-9e80-25c2aa58f53f.png#align=left&amp;display=inline&amp;height=45&amp;name=image.png&amp;originHeight=61&amp;originWidth=406&amp;size=13134&amp;width=297\" style=\"max-width: 600px; width: 297px;\" /></p><p><br /></p><h4 id=\"44ae3b04\">切比雪夫距离</h4><p style=\"text-align: center;\"><span class=\"lake-fontsize-11\" style=\"color: #333333;\"><img alt=\"image.png\" title=\"image.png\" src=\"https://cdn.nlark.com/yuque/0/2019/png/85998/1549871333600-ec675172-273c-44b6-89df-f32bad8703b9.png#align=left&amp;display=inline&amp;height=30&amp;name=image.png&amp;originHeight=29&amp;originWidth=305&amp;size=9221&amp;width=308\" style=\"max-width: 600px; width: 308px;\" /></span></p><p><span class=\"lake-fontsize-11\" style=\"color: #333333;\"><br /></span></p><p><span class=\"lake-fontsize-11\" style=\"color: #333333;\">不同的距离定义有不同的适用范围，一般来说：</span></p><ul><li><span class=\"lake-fontsize-11\" style=\"color: #333333;\">绝对距离直接把两个样本对应指标差值取绝对值求和，可谓简单粗暴；</span></li><li><span class=\"lake-fontsize-11\" style=\"color: #333333;\">欧几里得距离通常采用的是原始数据，而并非正规化后的数据，比如某个指标在10-100内取值，那么便可以直接使用，不需要将其规范到[0,1]区间再度量，否则便消除欧几里得距离的意义了；</span></li><li><span class=\"lake-fontsize-11\" style=\"color: #333333;\">闵科夫斯基距离把欧几里得距离一般化，应对高维情况；</span></li><li><span class=\"lake-fontsize-11\" style=\"color: #333333;\">切比雪夫距离呢？因为取了最大值，故两个样本最大差异最后归根到底可能落在少数个指标上，打个比方，我们要对比班里两个男同学，他们都很帅，家境都很好，成绩都很优异，但是，一个是痞子性格，一个一本正经性格，于是乎用切比雪夫距离来衡量可能就在他们性格上.</span></li></ul><p><br /></p><p><br /></p><p><br /></p>",
  "slug": 1235621,
  "title": "K-mean 算法"
}